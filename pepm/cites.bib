@inproceedings{lazyevaluator,
author = {Henderson, Peter and Morris, James H.},
title = {A Lazy Evaluator},
year = {1976},
Caisbn = {9781450374774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800168.811543},
doi = {10.1145/800168.811543},
abstract = {A different way to execute pure LISP programs is presented. It delays the evaluation of parameters and list structures without ever having to perform more evaluation steps than the usual method. Although the central idea can be found in earlier work this paper is of interest since it treats a rather well-known language and works out an algorithm which avoids full substitution. A partial correctness proof using Scott-Strachey semantics is sketched in a later section.},
booktitle = {Proceedings of the 3rd ACM SIGACT-SIGPLAN Symposium on Principles on Programming Languages},
pages = {95–103},
numpages = {9},
location = {Atlanta, Georgia},
series = {POPL '76}
}

@InProceedings{futamura,
author="Futamura, Yoshihiko",
editor="Goto, Eiichi
and Furukawa, Koichi
and Nakajima, Reiji
and Nakata, Ikuo
and Yonezawa, Akinori",
title="Partial computation of programs",
booktitle="RIMS Symposia on Software Science and Engineering",
year="1983",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--35",
abstract="This paper attempts to clarify the difference between partial and ordinary computation. Partial computation of a computer program is by definition ``specializing a general program based upon its operating environment into a more efficient program''. It also shows the usefulness of partial computation. Finally, the formal theory of partial computation, technical problems in making it practical, and its future research problems are discussed.",
isbn="978-3-540-39442-6"
}

@InProceedings{10.1007/978-3-540-25937-4_24,
author="Rogaway, Phillip
and Shrimpton, Thomas",
editor="Roy, Bimal
and Meier, Willi",
title="Cryptographic Hash-Function Basics: Definitions, Implications, and Separations for Preimage Resistance, Second-Preimage Resistance, and Collision Resistance",
booktitle="Fast Software Encryption",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="371--388",
abstract="We consider basic notions of security for cryptographic hash functions: collision resistance, preimage resistance, and second-preimage resistance. We give seven different definitions that correspond to these three underlying ideas, and then we work out all of the implications and separations among these seven definitions within the concrete-security, provable-security framework. Because our results are concrete, we can show two types of implications, conventional and provisional, where the strength of the latter depends on the amount of compression achieved by the hash function. We also distinguish two types of separations, conditional and unconditional. When constructing counterexamples for our separations, we are careful to preserve specified hash-function domains and ranges; this rules out some pathological counterexamples and makes the separations more meaningful in practice. Four of our definitions are standard while three appear to be new; some of our relations and separations have appeared, others have not. Here we give a modern treatment that acts to catalog, in one place and with carefully-considered nomenclature, the most basic security notions for cryptographic hash functions.",
isbn="978-3-540-25937-4"
}

@Article{Klotz2013,
author={Klotz, Johannes Georg
and Bossert, Martin
and Schober, Steffen},
title={Computing preimages of Boolean networks},
journal={BMC Bioinformatics},
year={2013},
month={Aug},
day={12},
volume={14},
number={10},
pages={S4},
abstract={In this paper we present an algorithm based on the sum-product algorithm that finds elements in the preimage of a feed-forward Boolean networks given an output of the network. Our probabilistic method runs in linear time with respect to the number of nodes in the network. We evaluate our algorithm for randomly constructed Boolean networks and a regulatory network of Escherichia coli and found that it gives a valid solution in most cases.},
issn={1471-2105},
doi={10.1186/1471-2105-14-S10-S4},
url={https://doi.org/10.1186/1471-2105-14-S10-S4}
}

@article{akutsu2009analyses,
  title={Analyses and algorithms for predecessor and control problems for Boolean networks of bounded indegree},
  author={Akutsu, Tatsuya and Hayashida, Morihiro and Zhang, Shu-Qin and Ching, Wai-Ki and Ng, Michael K},
  journal={Information and Media Technologies},
  volume={4},
  number={2},
  pages={338--349},
  year={2009},
  publisher={Information and Media Technologies Editorial Board}
}

@ARTICLE{1353287,  author={Kwok, J.T.-Y. and Tsang, I.W.-H.},  journal={IEEE Transactions on Neural Networks},   title={The pre-image problem in kernel methods},   year={2004},  volume={15},  number={6},  pages={1517-1525},  doi={10.1109/TNN.2004.837781}}


@Article{sym13040586,
AUTHOR = {Barnett, Stephen M. and Jeffers, John and Pegg, David T.},
TITLE = {Quantum Retrodiction: Foundations and Controversies},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {586},
URL = {https://www.mdpi.com/2073-8994/13/4/586},
ISSN = {2073-8994},
ABSTRACT = {Prediction is the making of statements, usually probabilistic, about future events based on current information. Retrodiction is the making of statements about past events based on current information. We present the foundations of quantum retrodiction and highlight its intimate connection with the Bayesian interpretation of probability. The close link with Bayesian methods enables us to explore controversies and misunderstandings about retrodiction that have appeared in the literature. To be clear, quantum retrodiction is universally applicable and draws its validity directly from conventional predictive quantum theory coupled with Bayes’ theorem.},
DOI = {10.3390/sym13040586}
}

@article{RevModPhys.27.179,
  title = {Symmetry of Physical Laws. {Part III}. Prediction and Retrodiction},
  author = {Watanabe, Satosi},
  journal = {Rev. Mod. Phys.},
  volume = {27},
  issue = {2},
  pages = {179--186},
  numpages = {0},
  year = {1955},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.27.179},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.27.179}
}

@Inbook{Aharonov2008,
author="Aharonov, Yakir
and Vaidman, Lev",
editor="Muga, J.G.
and Mayato, R. Sala
and Egusquiza, {\'I}.L.",
title="The Two-State Vector Formalism: An Updated Review",
bookTitle="Time in Quantum Mechanics",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="399--447",
isbn="978-3-540-73473-4",
doi="10.1007/978-3-540-73473-4_13",
url="https://doi.org/10.1007/978-3-540-73473-4_13"
}

@article{PhysRevA.54.147,
  title = {Quantum networks for elementary arithmetic operations},
  author = {Vedral, Vlatko and Barenco, Adriano and Ekert, Artur},
  journal = {Phys. Rev. A},
  volume = {54},
  issue = {1},
  pages = {147--153},
  numpages = {0},
  year = {1996},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.54.147},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.54.147}
}


@article{quteprints21763,
       publisher = {The University of Queensland Press},
          author = {Linda Burnett and William Millan and Edward Dawson and Andrew Clark},
            year = {2004},
           pages = {231--247},
         journal = {Australasian Journal of Combinatorics},
           title = {Simpler Methods for Generating Better Boolean Functions with Good Cryptographic Properties},
          volume = {29},
             url = {https://eprints.qut.edu.au/21763/}
}


@incollection{TOKAREVA20151,
title = {Chapter 1 - {Boolean} Functions},
editor = {Natalia Tokareva},
booktitle = {Bent Functions},
publisher = {Academic Press},
address = {Boston},
pages = {1-15},
year = {2015},
isbn = {978-0-12-802318-1},
doi = {https://doi.org/10.1016/B978-0-12-802318-1.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023181000017},
author = {Natalia Tokareva},
keywords = {Boolean function, Vectorial function, Algebraic normal form, Boolean cube, Hamming distance, Extended affine equivalence, Walsh-Hadamard transform, Finite field, Polynomial form, Trace form, Monomial function},
abstract = {In this chapter, we start with basic definitions related to Boolean functions. We consider the algebraic normal form of a Boolean function and the representation of a Boolean function over the Boolean cube. Extended affinely equivalent Boolean functions are defined as is the Walsh-Hadamard transform of a Boolean function. The finite field over F2 and its automorphisms are considered. It is shown how to associate Boolean functions in n variables with functions over the field F2n. We discuss polynomial representations of Boolean and vectorial Boolean functions. Representations of a Boolean function in the trace form and in the reduced trace form are given. Some details on the degree of a Boolean function in the trace form and on monomial functions are presented. The notions introduced in this chapter will be useful throughout the book.}
}

@inproceedings{10.1145/800027.808445,
author = {Boyer, Robert S. and Elspas, Bernard and Levitt, Karl N.},
title = {SELECT—a Formal System for Testing and Debugging Programs by Symbolic Execution},
year = {1975},
isbn = {9781450373852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800027.808445},
doi = {10.1145/800027.808445},
abstract = {SELECT is an experimental system for assisting in the formal systematic debugging of programs. It is intended to be a compromise between an automated program proving system and the current ad hoc debugging practice, and is similar to a system being developed by King et al. of IBM. SELECT systematically handles the paths of programs written in a LISP subset that includes arrays. For each execution path SELECT returns simplified conditions on input variables that cause the path to be executed, and simplified symbolic values for program variables at the path output. For conditions which form a system of linear equalities and inequalities SELECT will return input variable values that can serve as sample test data. The user can insert constraint conditions, at any point in the program including the output, in the form of symbolically executable assertions. These conditions can induce the system to select test data in user-specified regions. SELECT can also determine if the path is correct with respect to an output assertion. We present four examples demonstrating the various modes of system operation and their effectiveness in finding bugs. In some examples, SELECT was successful in automatically finding useful test data. In others, user interaction was required in the form of output assertions. SELECT appears to be a useful tool for rapidly revealing program errors, but for the future there is a need to expand its expressive and deductive power.},
booktitle = {Proceedings of the International Conference on Reliable Software},
pages = {234–245},
numpages = {12},
keywords = {Solution of systems of inequalities, Program debugging, Symbolic execution, Program testing, Program verification, Test data generation},
location = {Los Angeles, California}
}

@article{10.1145/390016.808445,
author = {Boyer, Robert S. and Elspas, Bernard and Levitt, Karl N.},
title = {SELECT—a Formal System for Testing and Debugging Programs by Symbolic Execution},
year = {1975},
issue_date = {June 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/390016.808445},
doi = {10.1145/390016.808445},
abstract = {SELECT is an experimental system for assisting in the formal systematic debugging of programs. It is intended to be a compromise between an automated program proving system and the current ad hoc debugging practice, and is similar to a system being developed by King et al. of IBM. SELECT systematically handles the paths of programs written in a LISP subset that includes arrays. For each execution path SELECT returns simplified conditions on input variables that cause the path to be executed, and simplified symbolic values for program variables at the path output. For conditions which form a system of linear equalities and inequalities SELECT will return input variable values that can serve as sample test data. The user can insert constraint conditions, at any point in the program including the output, in the form of symbolically executable assertions. These conditions can induce the system to select test data in user-specified regions. SELECT can also determine if the path is correct with respect to an output assertion. We present four examples demonstrating the various modes of system operation and their effectiveness in finding bugs. In some examples, SELECT was successful in automatically finding useful test data. In others, user interaction was required in the form of output assertions. SELECT appears to be a useful tool for rapidly revealing program errors, but for the future there is a need to expand its expressive and deductive power.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {234–245},
numpages = {12},
keywords = {Program debugging, Program verification, Test data generation, Symbolic execution, Program testing, Solution of systems of inequalities}
}


@article{10.1145/360248.360252,
author = {King, James C.},
title = {Symbolic Execution and Program Testing},
year = {1976},
issue_date = {July 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/360248.360252},
doi = {10.1145/360248.360252},
abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
journal = {Commun. ACM},
month = {jul},
pages = {385–394},
numpages = {10},
keywords = {symbolic execution, program proving, symbolic interpretation, program verification, program testing, program debugging}
}

@InProceedings{howden,
  author = 	 {William E. Howden},
  title = 	 {Experiments with a symbolic evaluation system},
  booktitle = {Proceedings of the National Computer Conference},
  year = 	 1976}

@inproceedings{10.1145/800191.805647,
author = {Clarke, Lori A.},
title = {A Program Testing System},
year = {1976},
isbn = {9781450374897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800191.805647},
doi = {10.1145/800191.805647},
abstract = {A system that aids in testing programs is described. This system symbolically executes program paths and creates symbolic representations of the output variables that aid in verifying a path's computations. The conditional statements that affect the flow of control are also symbolically represented by a set of inequalities. The inequalities are then evaluated to determine input data that would cause execution of the path. The system also does extensive error checking by simulating possible data dependent errors and then attempting to detect data sets that would cause execution errors.},
booktitle = {Proceedings of the 1976 Annual Conference},
pages = {488–491},
numpages = {4},
location = {Houston, Texas, USA},
series = {ACM '76}
}

@article{10.1145/3182657,
author = {Baldoni, Roberto and Coppa, Emilio and D’elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
title = {A Survey of Symbolic Execution Techniques},
year = {2018},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3182657},
doi = {10.1145/3182657},
abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program’s authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the past four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {50},
numpages = {39},
keywords = {static analysis, software testing, Symbolic execution, concolic execution}
}

@Article{djdeq,
  author = 	 {Alastair A. Abbott},
  title = 	 {The {Deutsch-Jozsa} problem: de-quantization and entanglement},
  journal = 	 {Natural Computing},
  year = 	 2012,
  volume = 	 11}

@article{10.1145/3341106,
author = {Komargodski, Ilan and Naor, Moni and Yogev, Eylon},
title = {White-Box vs. Black-Box Complexity of Search Problems: Ramsey and Graph Property Testing},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/3341106},
doi = {10.1145/3341106},
abstract = {Ramsey theory assures us that in any graph there is a clique or independent set of a certain size, roughly logarithmic in the graph size. But how difficult is it to find the clique or independent set? If the graph is given explicitly, then it is possible to do so while examining a linear number of edges. If the graph is given by a black-box, where to figure out whether a certain edge exists the box should be queried, then a large number of queries must be issued. But what if one is given a program or circuit for computing the existence of an edge? This problem was raised by Buss and Goldberg and Papadimitriou in the context of TFNP, search problems with a guaranteed solution.We examine the relationship between black-box complexity and white-box complexity for search problems with guaranteed solution such as the above Ramsey problem. We show that under the assumption that collision-resistant hash function exists (which follows from the hardness of problems such as factoring, discrete-log, and learning with errors) the white-box Ramsey problem is hard and this is true even if one is looking for a much smaller clique or independent set than the theorem guarantees. This is also true for the colorful Ramsey problem where one is looking, say, for a monochromatic triangle.In general, one cannot hope to translate all black-box hardness for TFNP into white-box hardness: we show this by adapting results concerning the random oracle methodology and the impossibility of instantiating it.Another model we consider is that of succinct black-box, where the complexity of an algorithm is measured as a function of the description size of the object in the box (and no limitation on the computation time). In this case, we show that for all TFNP problems there is an efficient algorithm with complexity proportional to the description size of the object in the box times the solution size. However, for promise problems this is not the case.Finally, we consider the complexity of graph property testing in the white-box model. We show a property that is hard to test even when one is given the program for computing the graph (under the appropriate assumptions such as hardness of Decisional Diffie-Hellman). The hard property is whether the graph is a two-source extractor.},
journal = {J. ACM},
month = {jul},
articleno = {34},
numpages = {28},
keywords = {black-box hardness, Ramsey theory, white-box hardness}
}

@inproceedings{10.1145/800157.805047,
author = {Cook, Stephen A.},
title = {The Complexity of Theorem-Proving Procedures},
year = {1971},
isbn = {9781450374644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800157.805047},
doi = {10.1145/800157.805047},
abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
booktitle = {Proceedings of the Third Annual ACM Symposium on Theory of Computing},
pages = {151–158},
numpages = {8},
location = {Shaker Heights, Ohio, USA},
series = {STOC '71}
}

@Inbook{Karp1972,
author="Karp, Richard M.",
editor="Miller, Raymond E.
and Thatcher, James W.
and Bohlinger, Jean D.",
title="Reducibility among Combinatorial Problems",
bookTitle="Complexity of Computer Computations: Proceedings of a symposium on the Complexity of Computer Computations, held March 20--22, 1972, at the IBM Thomas J. Watson Research Center, Yorktown Heights, New York, and sponsored by the Office of Naval Research, Mathematics Program, IBM World Trade Corporation, and the IBM Research Mathematical Sciences Department",
year="1972",
publisher="Springer US",
address="Boston, MA",
pages="85--103",
abstract="A large class of computational problems involve the determination of properties of graphs, digraphs, integers, arrays of integers, finite families of finite sets, boolean formulas and elements of other countable domains. Through simple encodings from such domains into the set of words over a finite alphabet these problems can be converted into language recognition problems, and we can inquire into their computational complexity. It is reasonable to consider such a problem satisfactorily solved when an algorithm for its solution is found which terminates within a number of steps bounded by a polynomial in the length of the input. We show that a large number of classic unsolved problems of covering, matching, packing, routing, assignment and sequencing are equivalent, in the sense that either each of them possesses a polynomial-bounded algorithm or none of them does.",
isbn="978-1-4684-2001-2",
doi="10.1007/978-1-4684-2001-2_9",
url="https://doi.org/10.1007/978-1-4684-2001-2_9"
}

@ARTICLE{4640789,  author={Trakhtenbrot, B.A.},  journal={Annals of the History of Computing},   title={A Survey of Russian Approaches to Perebor (Brute-Force Searches) Algorithms},   year={1984},  volume={6},  number={4},  pages={384-400},  doi={10.1109/MAHC.1984.10036}}

@book{nielsen_chuang_2010, place={Cambridge}, title={Quantum Computation and Quantum Information: 10th Anniversary Edition}, DOI={10.1017/CBO9780511976667}, publisher={Cambridge University Press}, author={Nielsen, Michael A. and Chuang, Isaac L.}, year={2010}}


@article{doi:10.1137/S0097539795293172,
author = {Shor, Peter W.},
title = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
journal = {SIAM Journal on Computing},
volume = {26},
number = {5},
pages = {1484-1509},
year = {1997},
doi = {10.1137/S0097539795293172},

URL = { 
        https://doi.org/10.1137/S0097539795293172
    
},
eprint = { 
        https://doi.org/10.1137/S0097539795293172
    
}
,
    abstract = { A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored. }
}


@inproceedings{10.1145/237814.237866,
author = {Grover, Lov K.},
title = {A Fast Quantum Mechanical Algorithm for Database Search},
year = {1996},
isbn = {0897917855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/237814.237866},
doi = {10.1145/237814.237866},
booktitle = {Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing},
pages = {212–219},
numpages = {8},
location = {Philadelphia, Pennsylvania, USA},
series = {STOC '96}
}


@INPROCEEDINGS{365701,  author={Simon, D.R.},  booktitle={Proceedings 35th Annual Symposium on Foundations of Computer Science},   title={On the power of quantum computation},   year={1994},  volume={},  number={},  pages={116-123},  doi={10.1109/SFCS.1994.365701}}


@Article{deutsch,
  author = 	 {David Deutsch},
  title = 	 {Quantum theory, the Church–Turing principle and the universal quantum computer},
  journal = 	 {Proc. R. Soc. Lond. A 400},
  year = 	 1985}

@Article{deutschJozsa,
  author = 	 {David Deutsch and Richard Jozsa},
  title = 	 {Rapid solution of problems by quantum computation},
  journal = 	 {Proc. R. Soc. Lond. A 439},
  year = 	 1992}


@article{doi:10.1137/S0097539796300921,
author = {Bernstein, Ethan and Vazirani, Umesh},
title = {Quantum Complexity Theory},
journal = {SIAM Journal on Computing},
volume = {26},
number = {5},
pages = {1411-1473},
year = {1997},
doi = {10.1137/S0097539796300921},

URL = { 
        https://doi.org/10.1137/S0097539796300921
    
},
eprint = { 
        https://doi.org/10.1137/S0097539796300921
    
}
,
    abstract = { In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch's model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97--117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension.We also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that \$O(\log T)\$ bits of precision suffice to support a T step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. We give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church--Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class \$\BPP\$. The class \$\BQP\$ of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies \$\BPP \subseteq \BQP \subseteq \Ptime^{\SP}\$. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory. }
}


@Article{shorFermat,
  author = 	 {Geller, Michael R. and Zhou, Zhongyuan},
  title = 	 {Factoring 51 and 85 with 8 qubits},
  journal = 	 {Scientific Reports (3023)},
  year = 	 2013,
  volume = 	 3,
  number = 	 1}


@article{10.2307/3560059,
 ISSN = {13645021},
 URL = {http://www.jstor.org/stable/3560059},
 abstract = {For any quantum algorithm operating on pure states, we prove that the presence of multi-partite entanglement, with a number of parties that increases unboundedly with input size, is necessary if the quantum algorithm is to offer an exponential speed-up over classical computation. Furthermore, we prove that the algorithm can be efficiently simulated classically to within a prescribed tolerance η even if a suitably small amount of global entanglement is present. We explicitly identify the occurrence of increasing multi-partite entanglement in Shor's algorithm. Our results do not apply to quantum algorithms operating on mixed states in general and we discuss the suggestion that an exponential computational speed-up might be possible with mixed states in the total absence of entanglement. Finally, despite the essential role of entanglement for pure-state algorithms, we argue that it is nevertheless misleading to view entanglement as a key resource for quantum-computational power.},
 author = {Richard Jozsa and Noah Linden},
 journal = {Proceedings: Mathematical, Physical and Engineering Sciences},
 number = {2036},
 pages = {2011--2032},
 publisher = {The Royal Society},
 title = {On the Role of Entanglement in Quantum-Computational Speed-Up},
 urldate = {2022-04-20},
 volume = {459},
 year = {2003}
}


@article{10.5555/3179473.3179481,
author = {Bocharov, Alex and Cui, Shawn X. and Roetteler, Martin and Svore, Krysta M.},
title = {Improved Quantum Ternary Arithmetic},
year = {2016},
issue_date = {July 2016},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {16},
number = {9–10},
issn = {1533-7146},
abstract = {Qutrit (or ternary) structures arise naturally in many quantum systems, notably in certain non-abelian anyon systems. We present efficient circuits for ternary reversible and quantum arithmetics. Our main result is the derivation of circuits for two families of ternary quantumadders. The main distinction from the binary adders is a richer ternary carry which leads potentially to higher resource counts in universal ternary bases. Our ternary ripple adder circuit has a circuit depth of O(n) and uses only 1 ancilla, making it more efficient in both, circuit depth and width, when compared with previous constructions. Our ternary carry lookahead circuit has a circuit depth of only O(log n), while using O(n) ancillas. Our approach works on two levels of abstraction: at the first level, descriptions of arithmetic circuits are given in terms of gates sequences that use various types of non-Clifford reflections. At the second level, we break down these reflections further by deriving them either from the two-qutrit Clifford gates and the non-Clifford gate C(X) : |i, j〉 → |i, j + δi,2 mod 3〉 or from the two-qutrit Clifford gates and the non-Clifford gate P9 = diag(e-2π i/9, 1, e2π i/9). The two choices of elementary gate sets correspond to two possible mappings onto two different prospective quantum computing architectures which we call the metaplectic and the supermetaplectic basis, respectively. Finally, we develop a method to factor diagonal unitaries using multi-variate polynomials over the ternary finite field which allows to characterize classes of gates that can be implemented exactly over the supermetaplectic basis.},
journal = {Quantum Info. Comput.},
month = {jul},
pages = {862–884},
numpages = {23},
keywords = {quantum adders, ternary quantum systems, quantum circuits}
}

@article{GE2004,
  title = {A Subsystem-Independent Generalization of Entanglement},
  author = {Barnum, Howard and Knill, Emanuel and Ortiz, Gerardo and Somma, Rolando and Viola, Lorenza},
  journal = {Phys. Rev. Lett.},
  volume = {92},
  issue = {10},
  pages = {107902},
  numpages = {4},
  year = {2004},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.92.107902},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.92.107902}
}

@book{10.5555/35517,
author = {Wegener, Ingo},
title = {The Complexity of Boolean Functions},
year = {1987},
isbn = {0471915556},
publisher = {John Wiley \& Sons, Inc.},
address = {USA}
}

@article{tagless,
    title={Finally tagless, partially evaluated: Tagless staged 
    interpreters for simpler typed languages},
    author={Carette, Jacques and Kiselyov, Oleg and Shan, Chung-chieh},
    journal={Journal of Functional Programming},
    volume={19},
    number={5},
    pages={509--543},
    year={2009},
    publisher={Cambridge University Press}
}

@article{retrodictive,
    title={Retrodictive Quantum Computing},
    author={Carette, Jacques and Ortiz, Gerardo and Sabry, Amr},
    journal={arXiv:2205.06346},
    volume={},
    number={},
    pages={},
    year={2022},
    publisher={}
}
@article{parnas1986rational,
  title={A rational design process: How and why to fake it},
  author={Parnas, David Lorge and Clements, Paul C},
  journal={IEEE transactions on software engineering},
  number={2},
  pages={251--257},
  year={1986},
  publisher={IEEE}
}

@inproceedings{soeken2016fast,
  title={A fast symbolic transformation based algorithm for reversible logic synthesis},
  author={Soeken, Mathias and Dueck, Gerhard W and Miller, D Michael},
  booktitle={International Conference on Reversible Computation},
  pages={307--321},
  year={2016},
  organization={Springer}
}

@InProceedings{10.1007/978-3-319-63390-9_1,
author="Amy, Matthew
and Roetteler, Martin
and Svore, Krysta M.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Verified Compilation of Space-Efficient Reversible Circuits",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="3--21",
abstract="The generation of reversible circuits from high-level code is an important problem in several application domains, including low-power electronics and quantum computing. Existing tools compile and optimize reversible circuits for various metrics, such as the overall circuit size or the total amount of space required to implement a given function reversibly. However, little effort has been spent on verifying the correctness of the results, an issue of particular importance in quantum computing. There, compilation allows not only mapping to hardware, but also the estimation of resources required to implement a given quantum algorithm, a process that is crucial for identifying which algorithms will outperform their classical counterparts. We present a reversible circuit compiler called ReVerC, which has been formally verified in F{\$}{\$}^{\backslash}star {\$}{\$}and compiles circuits that operate correctly with respect to the input program. Our compiler compiles the Revs language [21] to combinational reversible circuits with as few ancillary bits as possible, and provably cleans temporary values.",
isbn="978-3-319-63390-9"
}

@inproceedings{10.1145/1929501.1929506,
author = {Mogensen, Torben \AE{}gidius},
title = {Partial Evaluation of the Reversible Language Janus},
year = {2011},
isbn = {9781450304856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1929501.1929506},
doi = {10.1145/1929501.1929506},
abstract = {A reversible programming language is a programming language in which you can only write reversible programs, i.e., programs that can be run both forwards (computing outputs from inputs) and backwards (computing inputs from outputs). It is interesting to study reversible programs and languages because computations on reversible computers (computers that only allow reversible programs) in theory can be done using less energy than computations on traditional irreversible computers. Janus is a reversible, structured imperative programming language.We present a partial evaluator for the full Janus language with the exception of procedure calls. The partial evaluator converts Janus programs into reversible flowcharts, specialises these using polyvariant specialisation and converts the result back to structured form. Reversibility adds some complications, which we address in the paper. We demonstrate the results by some small examples.We believe this to be the first partial evaluator for a deterministic reversible programming language.},
booktitle = {Proceedings of the 20th ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
pages = {23–32},
numpages = {10},
keywords = {reversible computation, parial evaluation, janus},
location = {Austin, Texas, USA},
series = {PEPM '11}
}

@article{10.1145/3563309,
author = {Li, Liyi and Voichick, Finn and Hietala, Kesha and Peng, Yuxiang and Wu, Xiaodi and Hicks, Michael},
title = {Verified Compilation of Quantum Oracles},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563309},
doi = {10.1145/3563309},
abstract = {Quantum algorithms often apply classical operations, such as arithmetic or predicate checks, over a quantum superposition of classical data; these so-called oracles are often the largest components of a quantum program. To ease the construction of efficient, correct oracle functions, this paper presents VQO, a high-assurance framework implemented with the Coq proof assistant. The core of VQO is OQASM, the oracle quantum assembly language. OQASM operations move qubits between two different bases via the quantum Fourier transform, thus admitting important optimizations, but without inducing entanglement and the exponential blowup that comes with it. OQASM’s design enabled us to prove correct VQO’s compilers—from a simple imperative language called OQIMP to OQASM, and from OQASM to SQIR, a general-purpose quantum assembly language—and allowed us to efficiently test properties of OQASM programs using the QuickChick property-based testing framework. We have used VQO to implement a variety of arithmetic and geometric operators that are building blocks for important oracles, including those used in Shor’s and Grover’s algorithms. We found that VQO’s QFT-based arithmetic oracles require fewer qubits, sometimes substantially fewer, than those constructed using “classical” gates; VQO’s versions of the latter were nevertheless on par with or better than (in terms of both qubit and gate counts) oracles produced by Quipper, a state-of-the-art but unverified quantum programming platform.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {146},
numpages = {27},
keywords = {Type System, Compiler Verification, Programming Language Design, Quantum Oracle}
}

@inproceedings{DBLP:journals/corr/abs-1805-06908,
  author    = {Matthew Amy},
  editor    = {Peter Selinger and
               Giulio Chiribella},
  title     = {Towards Large-scale Functional Verification of Universal Quantum Circuits},
  booktitle = {Proceedings 15th International Conference on Quantum Physics and Logic,
               {QPL} 2018, Halifax, Canada, 3-7th June 2018},
  series    = {{EPTCS}},
  volume    = {287},
  pages     = {1--21},
  year      = {2018},
  url       = {https://doi.org/10.4204/EPTCS.287.1},
  doi       = {10.4204/EPTCS.287.1},
  timestamp = {Wed, 29 May 2019 11:09:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-06908.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@ARTICLE{aharonov:toffolihadamard,
        author = {D. Aharonov},
        title = {A simple proof that {T}offoli and {H}adamard are qauntum universal},
        journal = {arXiv:quant-ph/0301040},
        year = {2003}
}


@article{osti_319738,
title = {The Heisenberg representation of quantum computers},
author = {Gottesman, D},
abstractNote = {Since Shor`s discovery of an algorithm to factor numbers on a quantum computer in polynomial time, quantum computation has become a subject of immense interest. Unfortunately, one of the key features of quantum computers--the difficulty of describing them on classical computers--also makes it difficult to describe and understand precisely what can be done with them. A formalism describing the evolution of operators rather than states has proven extremely fruitful in understanding an important class of quantum operations. States used in error correction and certain communication protocols can be described by their stabilizer, a group of tensor products of Pauli matrices. Even this simple group structure is sufficient to allow a rich range of quantum effects, although it falls short of the full power of quantum computation.},
doi = {},
url = {https://www.osti.gov/biblio/319738}, 
place = {United States},
year = {1998},
month = {6}
}

@Unpublished{aqft,
  author = 	 {D. Coppersmith},
  title = 	 {An approximate Fourier transform useful in quantum factoring},
  note = 	 {\url{arXiv:quant-ph/0201067}},
  year = 	 2002}

@Article{calude,
  author = 	 {Cristian S. Calude},
  title = 	 {De-quantizing the solution of {Deutsch}'s problem},
  journal = 	 {International Journal of Quantum Information},
  year = 	 2007,
  volume = 	 5,
  number = 	 3,
  pages = 	 {409-415}}



@article{PhysRevA.76.042321,
  title = {Efficient classical simulation of the approximate quantum {F}ourier transform},
  author = {Yoran, Nadav and Short, Anthony J.},
  journal = {Phys. Rev. A},
  volume = {76},
  issue = {4},
  pages = {042321},
  numpages = {5},
  year = {2007},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.76.042321},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.76.042321}
}


@Unpublished{qfft,
  author = 	 {Dorit Aharonov, Zeph Landau, Johann Makowsky},
  title = 	 {The quantum {FFT} can be classically simulated},
  note = 	 {\url{arXiv:quant-ph/0611156}},
  year = 	 2006}


@article{Browne_2007,
doi = {10.1088/1367-2630/9/5/146},
url = {https://dx.doi.org/10.1088/1367-2630/9/5/146},
year = {2007},
month = {may},
publisher = {},
volume = {9},
number = {5},
pages = {146},
author = {Daniel E Browne},
title = {Efficient classical simulation of the quantum {F}ourier transform},
journal = {New Journal of Physics},
abstract = {A number of elegant approaches have been developed for the identification of quantum circuits which can be efficiently simulated on a classical computer. Recently, these methods have been employed to demonstrate the classical simulability of the quantum Fourier transform (QFT). Here we show that one can demonstrate a number of simulability results for QFT circuits in a straightforward manner using Griffiths and Niu's semi-classical QFT construction (Griffiths and Niu 1996 Phys. Rev. Lett. 76 3228). We use this to analyse the simulability properties of the QFT with a variety of classes of entangled input states. We then discuss the consequences of these results in the context of Shor's factorization algorithm.}
}

@article{10.5555/2535639.2535646,
author = {Van Den Nest, Maarten},
title = {Efficient Classical Simulations of Quantum {F}ourier Transforms and Normalizer Circuits over Abelian Groups},
year = {2013},
issue_date = {November 2013},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {13},
number = {11–12},
issn = {1533-7146},
abstract = {The quantum Fourier transform (QFT) is an important ingredient in various quantum algorithms which achieve superpolynomial speed-ups over classical computers. In this paper we study under which conditions the QFT can be simulated efficiently classically. We introduce a class of quantum circuits, called normalizer circuits: a normalizer circuit over a finite Abelian group is any quantum circuit comprising the QFT over the group, gates which compute automorphisms and gates which realize quadratic functions on the group. In our main result we prove that all normalizer circuits have polynomial-time classical simulations. The proof uses algorithms for linear diophantine equation solving and the monomial matrix formalism introduced in our earlier work. Our result generalizes the Gottesman-Knill theorem: in particular, Clifford circuits for d-level qudits arise as normalizer circuits over the group Zdm. We also highlight connections between normalizer circuits and Shor's factoring algorithm, and the Abelian hidden subgroup problem in general. Finally we prove that quantum factoring cannot be realized as a normalizer circuit owing to its modular exponentiation subroutine.},
journal = {Quantum Info. Comput.},
month = {nov},
pages = {1007–1037},
numpages = {31},
keywords = {fourier transform, classical simulation, clifford circuits}
}